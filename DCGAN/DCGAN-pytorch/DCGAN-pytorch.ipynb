{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as v_utils\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from skimage import io\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-9:\n",
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([transforms.CenterCrop(108),\n",
    "                           transforms.Scale(size=64),\n",
    "                            transforms.ToTensor()])\n",
    "\n",
    "trainset = dset.ImageFolder(root='/home/snu/study/GAN/DCGAN/DCGAN-tensorflow/data/',transform=trans)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,shuffle=True, num_workers=2,drop_last=True)\n",
    "\n",
    "train_data = iter(trainloader)\n",
    "data_length = trainset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(ch_in,ch_out,k_size=4,stride=2,pad=1,bn=True,momentum=0.9):\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(ch_in,ch_out,k_size,stride,padding=pad))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(ch_out,momentum=momentum))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv(ch_in, ch_out, k_size, stride, pad=1, bn=True,momentum=0.9):\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(ch_in,ch_out,k_size,stride,padding=pad))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(ch_out,momentum=momentum))\n",
    "    return nn.Sequential(*layers)\n",
    "    \n",
    "def normal_init(m,mean,std):\n",
    "    if isinstance(m,nn.ConvTranspose2d) or isinstance(m,nn.Conv2d):\n",
    "        m.weight.data.normal_(mean,std)\n",
    "        b.bias.data.zero_()\n",
    "        \n",
    "def bn_init(m):\n",
    "    if isinstance(m,nn.BatchNorm2d):\n",
    "        m.weight.data.fill(1)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self,dim_z=100,input_size=64,out_dim=128,colch=3):\n",
    "        super(G,self).__init__()\n",
    "        \n",
    "        self.dim_z = dim_z\n",
    "        self.input_size = input_size\n",
    "        self.out_dim = out_dim\n",
    "        self.colch = colch\n",
    "        \n",
    "        self.fc = nn.Linear(dim_z, out_dim*8*(int(input_size/16)**2))\n",
    "        self.deconv1 = deconv(out_dim*8,out_dim*4,4,)\n",
    "        self.deconv2 = deconv(out_dim*4,out_dim*2,4)\n",
    "        self.deconv3 = deconv(out_dim*2,out_dim,4)\n",
    "        self.deconv4 = deconv(out_dim,colch,4,bn=False,pad=1)\n",
    "        \n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self,mean=0.,std=0.02):\n",
    "        for m in self._modules:\n",
    "            normal_init(m,mean,std)\n",
    "            bn_init(m)\n",
    "        \n",
    "    def forward(self,z):\n",
    "\n",
    "            out = self.fc(z)\n",
    "            out = F.relu(out.view(-1,self.out_dim*8\n",
    "                                  ,int(self.input_size/16),int(self.input_size/16)))\n",
    "            out = F.relu(self.deconv1(out))\n",
    "            out = F.relu(self.deconv2(out))\n",
    "            out = F.relu(self.deconv3(out))\n",
    "            out = F.tanh(self.deconv4(out))\n",
    "            return out\n",
    "        \n",
    "class D(nn.Module):\n",
    "    def __init__(self,input_size=64,conv_dim=128, colch=3,leaky=0.2):\n",
    "        super(D,self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.colch = colch\n",
    "        self.conv_dim = conv_dim\n",
    "        self.leaky = leaky\n",
    "        \n",
    "        self.conv1 = conv(self.colch,conv_dim,4,2,pad=1,bn=False)\n",
    "        self.conv2 = conv(conv_dim,conv_dim*2,4,2,pad=1)\n",
    "        self.conv3 = conv(conv_dim*2,conv_dim*4,4,2,pad=1)\n",
    "        self.conv4 = conv(conv_dim*4,conv_dim*8,4,2,pad=1)\n",
    "        self.fc = nn.Linear(conv_dim*8*(int(input_size/16)**2),1)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        \n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self,mean=0.,std=0.02):\n",
    "        for m in self._modules:\n",
    "            normal_init(m,mean,std)\n",
    "            bn_init(m)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        out = F.leaky_relu(self.conv1(x),self.leaky)\n",
    "        out = F.leaky_relu(self.conv2(out),self.leaky)\n",
    "        out = F.leaky_relu(self.conv3(out),self.leaky)\n",
    "        out = F.leaky_relu(self.conv4(out),self.leaky)\n",
    "        out = out.view(-1,self.conv_dim*8*(int(self.input_size/16)**2))\n",
    "        out = self.fc(out)\n",
    "        out = self.sigm(out).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self,input_size=64,dim_z=100,colch=3,leaky=0.2,\n",
    "                end_g_dim=64,start_d_dim=64):\n",
    "        \n",
    "        self.input_size= input_size\n",
    "        self.dim_z = dim_z\n",
    "        self.colch = colch\n",
    "        self.leaky = leaky\n",
    "        self.end_g_dim = end_g_dim\n",
    "        self.start_d_dim = start_d_dim\n",
    "        \n",
    "        self.G = None\n",
    "        self.D = None\n",
    "        self.G_optim = None\n",
    "        self.D_optim = None\n",
    "        self.G_losses = []\n",
    "        self.D_losses = []\n",
    "        self.build_model()\n",
    "        \n",
    "        self.fixed_z = Variable(torch.Tensor(64,100).uniform_(-1,1)).cuda()\n",
    "    def build_model(self):\n",
    "        self.G = G(self.dim_z,self.input_size,self.end_g_dim,self.colch).cuda()\n",
    "        self.D = D(self.input_size,self.start_d_dim, self.colch, self.leaky).cuda()\n",
    "    \n",
    "    def denorm(self, x):\n",
    "        \"\"\"Convert range (-1, 1) to (0, 1)\"\"\"\n",
    "        out = (x + 1) / 2\n",
    "        return out.clamp(0, 1)\n",
    "\n",
    "    def train(self, data, batch_size=128,\n",
    "              epoch=10,lr=0.0002, check_step=100):\n",
    "            \n",
    "        batch_length = data.__len__()\n",
    "        \n",
    "        self.G_optim = torch.optim.Adam(self.G.parameters(),lr=lr,betas = [0.5,0.999])\n",
    "        self.D_optim = torch.optim.Adam(self.D.parameters(),lr=lr,betas = [0.5,0.999])\n",
    "        \n",
    "        loss_func = nn.BCELoss()\n",
    "        \n",
    "        counter = 0\n",
    "        \n",
    "        if not os.path.isdir(\"./dcgan_result\"):\n",
    "            os.mkdir(\"./dcgan_result\")\n",
    "\n",
    "        for e in range(epoch):\n",
    "            for i,[batch_x,_] in enumerate(data):\n",
    "\n",
    "                batch_x = Variable(batch_x).cuda()\n",
    "                # update D\n",
    "                z = Variable(torch.Tensor(batch_size,self.dim_z).uniform_(-1,1)).cuda()\n",
    "                \n",
    "                self.D_optim.zero_grad()\n",
    "                \n",
    "                G_fake = self.denorm(self.G.forward(z))\n",
    "                D_fake = self.D.forward(G_fake)\n",
    "                D_real = self.D.forward(batch_x)\n",
    "                \n",
    "                D_loss = (torch.sum(loss_func(D_fake,Variable(torch.zeros(batch_size)).cuda()))+\n",
    "                          torch.sum(loss_func(D_real,Variable(torch.ones(batch_size)).cuda())))\n",
    "\n",
    "                D_loss.backward(retain_variables=False) # retain_variables=False : 다른 backward를 위해서              \n",
    "                self.D_optim.step()\n",
    "                \n",
    "                # update G\n",
    "                z = Variable(torch.Tensor(batch_size,self.dim_z).uniform_(-1,1)).cuda()\n",
    "                \n",
    "                self.G_optim.zero_grad()\n",
    "                \n",
    "                G_fake = self.denorm(self.G.forward(z))\n",
    "                D_fake = self.D.forward(G_fake)\n",
    "                \n",
    "                G_loss = torch.sum(loss_func(D_fake,Variable(torch.ones(batch_size)).cuda()))\n",
    "                \n",
    "                G_loss.backward()              \n",
    "                self.G_optim.step()\n",
    "                \n",
    "                counter+=1\n",
    "                if counter % check_step == 0:\n",
    "                    \n",
    "                    self.D_losses.append(D_loss.cpu().data.numpy())\n",
    "                    self.G_losses.append(G_loss.cpu().data.numpy())\n",
    "                    \n",
    "                    print(\"Epoch [%d/%d], Step [%d/%d], D_loss : %.4f, G_loss : %.4f\"%(\n",
    "                      e,epoch,i,batch_length,D_loss.cpu().data.numpy(),G_loss.cpu().data.numpy()))\n",
    "                    \n",
    "                    view = self.denorm(self.G.forward(self.fixed_z))\n",
    "                    v_utils.save_image(view.data[0:64],\"./dcgan_result/gen_{}_{}.png\".format(e,i), nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_z = Variable(torch.rand(30,100)).cuda()\n",
    "dcgan = DCGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py:92: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Step [99/1582], D_loss : 0.5150, G_loss : 2.8774\n",
      "Epoch [0/100], Step [199/1582], D_loss : 0.8844, G_loss : 2.4530\n",
      "Epoch [0/100], Step [299/1582], D_loss : 1.0193, G_loss : 2.9846\n",
      "Epoch [0/100], Step [399/1582], D_loss : 1.3897, G_loss : 1.8557\n",
      "Epoch [0/100], Step [499/1582], D_loss : 0.9164, G_loss : 2.4483\n",
      "Epoch [0/100], Step [599/1582], D_loss : 0.6929, G_loss : 2.7136\n",
      "Epoch [0/100], Step [699/1582], D_loss : 0.6222, G_loss : 3.0657\n",
      "Epoch [0/100], Step [799/1582], D_loss : 1.0650, G_loss : 2.6618\n",
      "Epoch [0/100], Step [899/1582], D_loss : 1.1801, G_loss : 1.6856\n",
      "Epoch [0/100], Step [999/1582], D_loss : 1.0990, G_loss : 4.5262\n",
      "Epoch [0/100], Step [1099/1582], D_loss : 0.5806, G_loss : 3.7533\n",
      "Epoch [0/100], Step [1199/1582], D_loss : 0.6882, G_loss : 3.5573\n",
      "Epoch [0/100], Step [1299/1582], D_loss : 0.9715, G_loss : 1.9512\n",
      "Epoch [0/100], Step [1399/1582], D_loss : 1.0738, G_loss : 4.2090\n",
      "Epoch [0/100], Step [1499/1582], D_loss : 0.5945, G_loss : 2.7541\n",
      "Epoch [1/100], Step [17/1582], D_loss : 0.5979, G_loss : 2.5641\n",
      "Epoch [1/100], Step [117/1582], D_loss : 1.2039, G_loss : 6.6096\n",
      "Epoch [1/100], Step [217/1582], D_loss : 0.7377, G_loss : 3.2828\n",
      "Epoch [1/100], Step [317/1582], D_loss : 0.4902, G_loss : 4.1532\n",
      "Epoch [1/100], Step [417/1582], D_loss : 0.6624, G_loss : 4.7306\n",
      "Epoch [1/100], Step [517/1582], D_loss : 0.4662, G_loss : 3.3048\n",
      "Epoch [1/100], Step [617/1582], D_loss : 0.4621, G_loss : 2.8506\n",
      "Epoch [1/100], Step [717/1582], D_loss : 0.5252, G_loss : 3.4737\n",
      "Epoch [1/100], Step [817/1582], D_loss : 1.5371, G_loss : 3.0390\n",
      "Epoch [1/100], Step [917/1582], D_loss : 0.8888, G_loss : 1.2846\n",
      "Epoch [1/100], Step [1017/1582], D_loss : 0.4667, G_loss : 3.3402\n",
      "Epoch [1/100], Step [1117/1582], D_loss : 0.6420, G_loss : 3.8391\n",
      "Epoch [1/100], Step [1217/1582], D_loss : 0.5965, G_loss : 2.7012\n",
      "Epoch [1/100], Step [1317/1582], D_loss : 0.6791, G_loss : 1.8047\n",
      "Epoch [1/100], Step [1417/1582], D_loss : 0.8520, G_loss : 1.7637\n",
      "Epoch [1/100], Step [1517/1582], D_loss : 0.8719, G_loss : 3.4378\n",
      "Epoch [2/100], Step [35/1582], D_loss : 0.8289, G_loss : 1.5726\n",
      "Epoch [2/100], Step [135/1582], D_loss : 1.1445, G_loss : 1.3461\n",
      "Epoch [2/100], Step [235/1582], D_loss : 0.6158, G_loss : 2.4548\n",
      "Epoch [2/100], Step [335/1582], D_loss : 0.6471, G_loss : 3.2612\n",
      "Epoch [2/100], Step [435/1582], D_loss : 0.4554, G_loss : 2.7935\n",
      "Epoch [2/100], Step [535/1582], D_loss : 0.9156, G_loss : 1.6013\n",
      "Epoch [2/100], Step [635/1582], D_loss : 0.5799, G_loss : 3.6748\n",
      "Epoch [2/100], Step [735/1582], D_loss : 0.5135, G_loss : 2.2771\n",
      "Epoch [2/100], Step [835/1582], D_loss : 1.0739, G_loss : 1.2139\n",
      "Epoch [2/100], Step [935/1582], D_loss : 0.5556, G_loss : 2.5079\n",
      "Epoch [2/100], Step [1035/1582], D_loss : 0.5600, G_loss : 1.9286\n",
      "Epoch [2/100], Step [1135/1582], D_loss : 0.9299, G_loss : 1.7664\n",
      "Epoch [2/100], Step [1235/1582], D_loss : 1.1907, G_loss : 5.3268\n",
      "Epoch [2/100], Step [1335/1582], D_loss : 0.8492, G_loss : 3.6165\n",
      "Epoch [2/100], Step [1435/1582], D_loss : 0.7608, G_loss : 3.7708\n",
      "Epoch [2/100], Step [1535/1582], D_loss : 0.4611, G_loss : 2.9097\n",
      "Epoch [3/100], Step [53/1582], D_loss : 1.0664, G_loss : 1.3638\n",
      "Epoch [3/100], Step [153/1582], D_loss : 1.0022, G_loss : 4.2385\n",
      "Epoch [3/100], Step [253/1582], D_loss : 0.5214, G_loss : 3.3911\n",
      "Epoch [3/100], Step [353/1582], D_loss : 1.3061, G_loss : 0.7962\n",
      "Epoch [3/100], Step [453/1582], D_loss : 1.0558, G_loss : 1.5803\n",
      "Epoch [3/100], Step [553/1582], D_loss : 0.7401, G_loss : 2.0354\n",
      "Epoch [3/100], Step [653/1582], D_loss : 1.0287, G_loss : 1.6680\n",
      "Epoch [3/100], Step [753/1582], D_loss : 0.8297, G_loss : 1.9955\n",
      "Epoch [3/100], Step [853/1582], D_loss : 0.4720, G_loss : 2.2164\n",
      "Epoch [3/100], Step [953/1582], D_loss : 0.6417, G_loss : 2.2035\n",
      "Epoch [3/100], Step [1053/1582], D_loss : 0.4415, G_loss : 2.8075\n",
      "Epoch [3/100], Step [1153/1582], D_loss : 0.8951, G_loss : 1.7376\n",
      "Epoch [3/100], Step [1253/1582], D_loss : 1.1157, G_loss : 1.0656\n",
      "Epoch [3/100], Step [1353/1582], D_loss : 0.4283, G_loss : 3.2704\n",
      "Epoch [3/100], Step [1453/1582], D_loss : 0.5661, G_loss : 2.2814\n",
      "Epoch [3/100], Step [1553/1582], D_loss : 0.3679, G_loss : 3.4160\n",
      "Epoch [4/100], Step [71/1582], D_loss : 0.4935, G_loss : 2.4578\n",
      "Epoch [4/100], Step [171/1582], D_loss : 0.3189, G_loss : 4.4184\n",
      "Epoch [4/100], Step [271/1582], D_loss : 0.3084, G_loss : 2.8786\n",
      "Epoch [4/100], Step [371/1582], D_loss : 0.5058, G_loss : 1.3046\n",
      "Epoch [4/100], Step [471/1582], D_loss : 0.2925, G_loss : 2.5588\n",
      "Epoch [4/100], Step [571/1582], D_loss : 0.3611, G_loss : 3.9656\n",
      "Epoch [4/100], Step [671/1582], D_loss : 0.8422, G_loss : 2.7490\n",
      "Epoch [4/100], Step [771/1582], D_loss : 0.3498, G_loss : 3.2585\n",
      "Epoch [4/100], Step [871/1582], D_loss : 0.5116, G_loss : 2.1872\n",
      "Epoch [4/100], Step [971/1582], D_loss : 0.2568, G_loss : 2.4532\n",
      "Epoch [4/100], Step [1071/1582], D_loss : 0.2594, G_loss : 2.5044\n",
      "Epoch [4/100], Step [1171/1582], D_loss : 0.3861, G_loss : 3.4470\n",
      "Epoch [4/100], Step [1271/1582], D_loss : 0.3291, G_loss : 2.8116\n",
      "Epoch [4/100], Step [1371/1582], D_loss : 0.7508, G_loss : 2.2567\n",
      "Epoch [4/100], Step [1471/1582], D_loss : 0.3957, G_loss : 2.8461\n",
      "Epoch [4/100], Step [1571/1582], D_loss : 0.3587, G_loss : 5.0319\n",
      "Epoch [5/100], Step [89/1582], D_loss : 0.3412, G_loss : 5.0007\n",
      "Epoch [5/100], Step [189/1582], D_loss : 0.1891, G_loss : 4.2466\n",
      "Epoch [5/100], Step [289/1582], D_loss : 0.5419, G_loss : 3.8071\n",
      "Epoch [5/100], Step [389/1582], D_loss : 0.5659, G_loss : 2.1893\n",
      "Epoch [5/100], Step [489/1582], D_loss : 0.4862, G_loss : 4.5716\n",
      "Epoch [5/100], Step [589/1582], D_loss : 0.2613, G_loss : 2.4463\n",
      "Epoch [5/100], Step [689/1582], D_loss : 2.6444, G_loss : 0.8572\n",
      "Epoch [5/100], Step [789/1582], D_loss : 0.3789, G_loss : 4.4419\n",
      "Epoch [5/100], Step [889/1582], D_loss : 0.4105, G_loss : 3.7061\n",
      "Epoch [5/100], Step [989/1582], D_loss : 0.2971, G_loss : 3.5383\n",
      "Epoch [5/100], Step [1089/1582], D_loss : 0.2569, G_loss : 2.7349\n",
      "Epoch [5/100], Step [1189/1582], D_loss : 0.2822, G_loss : 2.6275\n",
      "Epoch [5/100], Step [1289/1582], D_loss : 0.2879, G_loss : 3.0961\n",
      "Epoch [5/100], Step [1389/1582], D_loss : 0.2980, G_loss : 4.1636\n",
      "Epoch [5/100], Step [1489/1582], D_loss : 0.1832, G_loss : 3.9660\n",
      "Epoch [6/100], Step [7/1582], D_loss : 0.2746, G_loss : 3.0373\n",
      "Epoch [6/100], Step [107/1582], D_loss : 1.6111, G_loss : 1.6966\n",
      "Epoch [6/100], Step [207/1582], D_loss : 0.2915, G_loss : 2.1448\n",
      "Epoch [6/100], Step [307/1582], D_loss : 0.2066, G_loss : 3.4132\n",
      "Epoch [6/100], Step [407/1582], D_loss : 0.1704, G_loss : 4.9842\n",
      "Epoch [6/100], Step [507/1582], D_loss : 0.3536, G_loss : 2.1868\n",
      "Epoch [6/100], Step [607/1582], D_loss : 0.2843, G_loss : 2.5516\n",
      "Epoch [6/100], Step [707/1582], D_loss : 0.1800, G_loss : 5.2389\n",
      "Epoch [6/100], Step [807/1582], D_loss : 0.1030, G_loss : 3.5458\n",
      "Epoch [6/100], Step [907/1582], D_loss : 0.3541, G_loss : 1.4403\n",
      "Epoch [6/100], Step [1007/1582], D_loss : 0.1850, G_loss : 3.0489\n",
      "Epoch [6/100], Step [1107/1582], D_loss : 0.2058, G_loss : 3.2599\n",
      "Epoch [6/100], Step [1207/1582], D_loss : 0.1915, G_loss : 3.6001\n",
      "Epoch [6/100], Step [1307/1582], D_loss : 0.1235, G_loss : 3.1222\n",
      "Epoch [6/100], Step [1407/1582], D_loss : 0.6690, G_loss : 6.1721\n",
      "Epoch [6/100], Step [1507/1582], D_loss : 0.6078, G_loss : 2.4265\n",
      "Epoch [7/100], Step [25/1582], D_loss : 0.1179, G_loss : 5.2888\n",
      "Epoch [7/100], Step [125/1582], D_loss : 0.8596, G_loss : 3.8188\n",
      "Epoch [7/100], Step [225/1582], D_loss : 0.1591, G_loss : 4.5129\n",
      "Epoch [7/100], Step [325/1582], D_loss : 0.2033, G_loss : 3.0237\n",
      "Epoch [7/100], Step [425/1582], D_loss : 0.1119, G_loss : 3.9157\n",
      "Epoch [7/100], Step [525/1582], D_loss : 0.1844, G_loss : 3.2714\n",
      "Epoch [7/100], Step [625/1582], D_loss : 0.1256, G_loss : 4.4691\n",
      "Epoch [7/100], Step [725/1582], D_loss : 0.1608, G_loss : 3.4185\n",
      "Epoch [7/100], Step [825/1582], D_loss : 0.1475, G_loss : 4.1192\n",
      "Epoch [7/100], Step [925/1582], D_loss : 0.1117, G_loss : 4.9382\n",
      "Epoch [7/100], Step [1025/1582], D_loss : 0.1731, G_loss : 4.2082\n",
      "Epoch [7/100], Step [1125/1582], D_loss : 0.2638, G_loss : 3.9040\n",
      "Epoch [7/100], Step [1225/1582], D_loss : 0.1863, G_loss : 4.5309\n",
      "Epoch [7/100], Step [1325/1582], D_loss : 2.4876, G_loss : 9.8171\n",
      "Epoch [7/100], Step [1425/1582], D_loss : 0.3765, G_loss : 7.3612\n",
      "Epoch [7/100], Step [1525/1582], D_loss : 0.1639, G_loss : 6.3706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Step [43/1582], D_loss : 0.9185, G_loss : 9.8999\n",
      "Epoch [8/100], Step [143/1582], D_loss : 0.1718, G_loss : 4.2329\n",
      "Epoch [8/100], Step [243/1582], D_loss : 0.2007, G_loss : 2.1357\n",
      "Epoch [8/100], Step [343/1582], D_loss : 0.8263, G_loss : 2.8504\n",
      "Epoch [8/100], Step [443/1582], D_loss : 0.1143, G_loss : 3.8977\n",
      "Epoch [8/100], Step [543/1582], D_loss : 0.1683, G_loss : 4.8332\n",
      "Epoch [8/100], Step [643/1582], D_loss : 0.6274, G_loss : 5.0931\n",
      "Epoch [8/100], Step [743/1582], D_loss : 0.1399, G_loss : 3.3666\n",
      "Epoch [8/100], Step [843/1582], D_loss : 0.2299, G_loss : 3.1330\n",
      "Epoch [8/100], Step [943/1582], D_loss : 0.0630, G_loss : 4.8439\n",
      "Epoch [8/100], Step [1043/1582], D_loss : 0.1654, G_loss : 3.6241\n",
      "Epoch [8/100], Step [1143/1582], D_loss : 0.4267, G_loss : 4.1782\n",
      "Epoch [8/100], Step [1243/1582], D_loss : 0.1558, G_loss : 3.7102\n",
      "Epoch [8/100], Step [1343/1582], D_loss : 0.7877, G_loss : 4.9013\n",
      "Epoch [8/100], Step [1443/1582], D_loss : 0.1616, G_loss : 5.0518\n",
      "Epoch [8/100], Step [1543/1582], D_loss : 0.2746, G_loss : 6.2055\n",
      "Epoch [9/100], Step [61/1582], D_loss : 0.1097, G_loss : 5.2851\n",
      "Epoch [9/100], Step [161/1582], D_loss : 0.1819, G_loss : 4.5285\n",
      "Epoch [9/100], Step [261/1582], D_loss : 0.0659, G_loss : 4.4251\n",
      "Epoch [9/100], Step [361/1582], D_loss : 1.3401, G_loss : 9.3875\n",
      "Epoch [9/100], Step [461/1582], D_loss : 0.1548, G_loss : 2.6918\n",
      "Epoch [9/100], Step [561/1582], D_loss : 0.2220, G_loss : 4.5544\n",
      "Epoch [9/100], Step [661/1582], D_loss : 0.0509, G_loss : 3.8723\n",
      "Epoch [9/100], Step [761/1582], D_loss : 0.0753, G_loss : 4.6839\n",
      "Epoch [9/100], Step [861/1582], D_loss : 0.5464, G_loss : 3.1877\n",
      "Epoch [9/100], Step [961/1582], D_loss : 0.1928, G_loss : 4.5113\n",
      "Epoch [9/100], Step [1061/1582], D_loss : 0.2987, G_loss : 5.7175\n",
      "Epoch [9/100], Step [1161/1582], D_loss : 0.0956, G_loss : 3.0788\n",
      "Epoch [9/100], Step [1261/1582], D_loss : 0.3481, G_loss : 7.0196\n",
      "Epoch [9/100], Step [1361/1582], D_loss : 0.0283, G_loss : 5.2188\n",
      "Epoch [9/100], Step [1461/1582], D_loss : 0.1565, G_loss : 4.0585\n",
      "Epoch [9/100], Step [1561/1582], D_loss : 0.1908, G_loss : 4.0925\n",
      "Epoch [10/100], Step [79/1582], D_loss : 0.2999, G_loss : 3.1804\n",
      "Epoch [10/100], Step [179/1582], D_loss : 0.1087, G_loss : 4.6935\n",
      "Epoch [10/100], Step [279/1582], D_loss : 0.0773, G_loss : 4.7397\n",
      "Epoch [10/100], Step [379/1582], D_loss : 0.1826, G_loss : 3.3473\n",
      "Epoch [10/100], Step [479/1582], D_loss : 0.1711, G_loss : 4.7124\n",
      "Epoch [10/100], Step [579/1582], D_loss : 0.0561, G_loss : 4.3049\n",
      "Epoch [10/100], Step [679/1582], D_loss : 0.1535, G_loss : 5.0098\n",
      "Epoch [10/100], Step [779/1582], D_loss : 0.0607, G_loss : 4.2098\n",
      "Epoch [10/100], Step [879/1582], D_loss : 0.1225, G_loss : 5.5615\n",
      "Epoch [10/100], Step [979/1582], D_loss : 0.1461, G_loss : 6.6171\n",
      "Epoch [10/100], Step [1079/1582], D_loss : 0.0782, G_loss : 4.6260\n",
      "Epoch [10/100], Step [1179/1582], D_loss : 0.1810, G_loss : 4.8021\n",
      "Epoch [10/100], Step [1279/1582], D_loss : 0.0528, G_loss : 4.7638\n",
      "Epoch [10/100], Step [1379/1582], D_loss : 0.4364, G_loss : 5.6267\n",
      "Epoch [10/100], Step [1479/1582], D_loss : 0.1615, G_loss : 5.3226\n",
      "Epoch [10/100], Step [1579/1582], D_loss : 0.0515, G_loss : 4.6048\n",
      "Epoch [11/100], Step [97/1582], D_loss : 0.0877, G_loss : 4.8468\n",
      "Epoch [11/100], Step [197/1582], D_loss : 0.1195, G_loss : 4.6559\n",
      "Epoch [11/100], Step [297/1582], D_loss : 0.0641, G_loss : 6.0981\n",
      "Epoch [11/100], Step [397/1582], D_loss : 0.1289, G_loss : 4.6454\n",
      "Epoch [11/100], Step [497/1582], D_loss : 0.1233, G_loss : 4.7806\n",
      "Epoch [11/100], Step [597/1582], D_loss : 0.0637, G_loss : 4.3929\n",
      "Epoch [11/100], Step [697/1582], D_loss : 0.1030, G_loss : 4.5844\n",
      "Epoch [11/100], Step [797/1582], D_loss : 0.0655, G_loss : 5.1053\n",
      "Epoch [11/100], Step [897/1582], D_loss : 0.2504, G_loss : 1.6116\n",
      "Epoch [11/100], Step [997/1582], D_loss : 0.0398, G_loss : 6.3064\n",
      "Epoch [11/100], Step [1097/1582], D_loss : 0.3199, G_loss : 5.4287\n",
      "Epoch [11/100], Step [1197/1582], D_loss : 0.0569, G_loss : 4.2643\n",
      "Epoch [11/100], Step [1297/1582], D_loss : 0.0475, G_loss : 5.4195\n",
      "Epoch [11/100], Step [1397/1582], D_loss : 0.0918, G_loss : 4.2973\n",
      "Epoch [11/100], Step [1497/1582], D_loss : 0.2270, G_loss : 3.0156\n",
      "Epoch [12/100], Step [15/1582], D_loss : 0.2468, G_loss : 4.0594\n",
      "Epoch [12/100], Step [115/1582], D_loss : 0.0940, G_loss : 4.3637\n",
      "Epoch [12/100], Step [215/1582], D_loss : 0.0829, G_loss : 4.6383\n",
      "Epoch [12/100], Step [315/1582], D_loss : 0.5885, G_loss : 5.9443\n",
      "Epoch [12/100], Step [415/1582], D_loss : 4.7821, G_loss : 11.9809\n",
      "Epoch [12/100], Step [515/1582], D_loss : 0.0372, G_loss : 3.9543\n",
      "Epoch [12/100], Step [615/1582], D_loss : 0.2804, G_loss : 4.6210\n",
      "Epoch [12/100], Step [715/1582], D_loss : 4.4963, G_loss : 7.6567\n",
      "Epoch [12/100], Step [815/1582], D_loss : 0.2097, G_loss : 6.7372\n",
      "Epoch [12/100], Step [915/1582], D_loss : 0.2889, G_loss : 2.5232\n",
      "Epoch [12/100], Step [1015/1582], D_loss : 0.0455, G_loss : 4.5716\n",
      "Epoch [12/100], Step [1115/1582], D_loss : 0.2896, G_loss : 8.0579\n",
      "Epoch [12/100], Step [1215/1582], D_loss : 0.0598, G_loss : 4.9298\n",
      "Epoch [12/100], Step [1315/1582], D_loss : 0.0770, G_loss : 4.5036\n",
      "Epoch [12/100], Step [1415/1582], D_loss : 0.0883, G_loss : 5.5049\n",
      "Epoch [12/100], Step [1515/1582], D_loss : 0.0897, G_loss : 4.7298\n",
      "Epoch [13/100], Step [33/1582], D_loss : 0.1160, G_loss : 4.7204\n",
      "Epoch [13/100], Step [133/1582], D_loss : 0.0714, G_loss : 4.2732\n",
      "Epoch [13/100], Step [233/1582], D_loss : 0.1502, G_loss : 5.9752\n",
      "Epoch [13/100], Step [333/1582], D_loss : 0.0401, G_loss : 5.6590\n",
      "Epoch [13/100], Step [433/1582], D_loss : 0.4096, G_loss : 4.7554\n",
      "Epoch [13/100], Step [533/1582], D_loss : 0.1678, G_loss : 4.4078\n",
      "Epoch [13/100], Step [633/1582], D_loss : 0.1358, G_loss : 3.6593\n",
      "Epoch [13/100], Step [733/1582], D_loss : 0.0397, G_loss : 6.0371\n",
      "Epoch [13/100], Step [833/1582], D_loss : 0.0821, G_loss : 5.1751\n",
      "Epoch [13/100], Step [933/1582], D_loss : 0.1374, G_loss : 5.0675\n",
      "Epoch [13/100], Step [1033/1582], D_loss : 0.1153, G_loss : 5.9227\n",
      "Epoch [13/100], Step [1133/1582], D_loss : 0.0650, G_loss : 4.2920\n",
      "Epoch [13/100], Step [1233/1582], D_loss : 0.2694, G_loss : 3.2061\n",
      "Epoch [13/100], Step [1333/1582], D_loss : 0.5385, G_loss : 8.7603\n",
      "Epoch [13/100], Step [1433/1582], D_loss : 0.2267, G_loss : 4.1541\n",
      "Epoch [13/100], Step [1533/1582], D_loss : 0.1215, G_loss : 7.2442\n",
      "Epoch [14/100], Step [51/1582], D_loss : 0.1273, G_loss : 3.8191\n",
      "Epoch [14/100], Step [151/1582], D_loss : 1.2903, G_loss : 14.1866\n",
      "Epoch [14/100], Step [251/1582], D_loss : 0.0707, G_loss : 4.9770\n",
      "Epoch [14/100], Step [351/1582], D_loss : 0.0788, G_loss : 4.0152\n",
      "Epoch [14/100], Step [451/1582], D_loss : 0.0489, G_loss : 5.2286\n",
      "Epoch [14/100], Step [551/1582], D_loss : 0.0433, G_loss : 5.5988\n",
      "Epoch [14/100], Step [651/1582], D_loss : 0.3131, G_loss : 2.3430\n",
      "Epoch [14/100], Step [751/1582], D_loss : 0.2673, G_loss : 4.7040\n",
      "Epoch [14/100], Step [851/1582], D_loss : 0.1663, G_loss : 6.0048\n",
      "Epoch [14/100], Step [951/1582], D_loss : 0.1718, G_loss : 3.5134\n",
      "Epoch [14/100], Step [1051/1582], D_loss : 0.0608, G_loss : 4.7916\n",
      "Epoch [14/100], Step [1151/1582], D_loss : 0.0548, G_loss : 5.1823\n",
      "Epoch [14/100], Step [1251/1582], D_loss : 0.7043, G_loss : 5.4265\n",
      "Epoch [14/100], Step [1351/1582], D_loss : 0.1769, G_loss : 4.0042\n",
      "Epoch [14/100], Step [1451/1582], D_loss : 0.1649, G_loss : 5.6917\n",
      "Epoch [14/100], Step [1551/1582], D_loss : 0.0462, G_loss : 5.2837\n",
      "Epoch [15/100], Step [69/1582], D_loss : 0.0589, G_loss : 4.5147\n",
      "Epoch [15/100], Step [169/1582], D_loss : 0.3024, G_loss : 4.2886\n",
      "Epoch [15/100], Step [269/1582], D_loss : 0.0542, G_loss : 5.4437\n",
      "Epoch [15/100], Step [369/1582], D_loss : 0.1328, G_loss : 4.8682\n",
      "Epoch [15/100], Step [469/1582], D_loss : 0.0442, G_loss : 5.6440\n",
      "Epoch [15/100], Step [569/1582], D_loss : 1.1666, G_loss : 8.2099\n",
      "Epoch [15/100], Step [669/1582], D_loss : 0.2245, G_loss : 3.7225\n",
      "Epoch [15/100], Step [769/1582], D_loss : 0.0721, G_loss : 4.3054\n",
      "Epoch [15/100], Step [869/1582], D_loss : 0.1361, G_loss : 3.4217\n",
      "Epoch [15/100], Step [969/1582], D_loss : 0.1231, G_loss : 3.8279\n",
      "Epoch [15/100], Step [1069/1582], D_loss : 0.0511, G_loss : 4.7484\n",
      "Epoch [15/100], Step [1169/1582], D_loss : 0.0667, G_loss : 5.4846\n",
      "Epoch [15/100], Step [1269/1582], D_loss : 0.0789, G_loss : 5.0347\n",
      "Epoch [15/100], Step [1369/1582], D_loss : 0.0794, G_loss : 4.7262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Step [1469/1582], D_loss : 0.0734, G_loss : 4.6518\n",
      "Epoch [15/100], Step [1569/1582], D_loss : 0.3964, G_loss : 4.2904\n",
      "Epoch [16/100], Step [87/1582], D_loss : 0.0405, G_loss : 4.7539\n",
      "Epoch [16/100], Step [187/1582], D_loss : 0.0664, G_loss : 5.1467\n",
      "Epoch [16/100], Step [287/1582], D_loss : 0.1674, G_loss : 4.8840\n",
      "Epoch [16/100], Step [387/1582], D_loss : 0.0580, G_loss : 4.7091\n",
      "Epoch [16/100], Step [487/1582], D_loss : 0.2944, G_loss : 4.3041\n",
      "Epoch [16/100], Step [587/1582], D_loss : 0.1724, G_loss : 4.6200\n",
      "Epoch [16/100], Step [687/1582], D_loss : 0.1567, G_loss : 6.2122\n",
      "Epoch [16/100], Step [787/1582], D_loss : 0.1795, G_loss : 6.3690\n",
      "Epoch [16/100], Step [887/1582], D_loss : 0.1432, G_loss : 4.1607\n",
      "Epoch [16/100], Step [987/1582], D_loss : 0.0923, G_loss : 5.5489\n",
      "Epoch [16/100], Step [1087/1582], D_loss : 0.2592, G_loss : 3.1958\n",
      "Epoch [16/100], Step [1187/1582], D_loss : 0.0768, G_loss : 4.8534\n",
      "Epoch [16/100], Step [1287/1582], D_loss : 0.0806, G_loss : 4.7562\n",
      "Epoch [16/100], Step [1387/1582], D_loss : 0.0945, G_loss : 6.1409\n",
      "Epoch [16/100], Step [1487/1582], D_loss : 0.1269, G_loss : 4.4156\n",
      "Epoch [17/100], Step [5/1582], D_loss : 0.0652, G_loss : 3.7889\n",
      "Epoch [17/100], Step [105/1582], D_loss : 0.1901, G_loss : 5.2929\n",
      "Epoch [17/100], Step [205/1582], D_loss : 0.1331, G_loss : 3.6150\n",
      "Epoch [17/100], Step [305/1582], D_loss : 0.1223, G_loss : 4.7079\n",
      "Epoch [17/100], Step [405/1582], D_loss : 0.0896, G_loss : 3.5028\n",
      "Epoch [17/100], Step [505/1582], D_loss : 0.3237, G_loss : 5.4430\n",
      "Epoch [17/100], Step [605/1582], D_loss : 0.3616, G_loss : 4.6838\n",
      "Epoch [17/100], Step [705/1582], D_loss : 0.1378, G_loss : 4.4865\n",
      "Epoch [17/100], Step [805/1582], D_loss : 1.0009, G_loss : 7.6194\n",
      "Epoch [17/100], Step [905/1582], D_loss : 0.0482, G_loss : 5.2752\n",
      "Epoch [17/100], Step [1005/1582], D_loss : 0.0518, G_loss : 5.3090\n",
      "Epoch [17/100], Step [1105/1582], D_loss : 0.0466, G_loss : 4.8276\n",
      "Epoch [17/100], Step [1205/1582], D_loss : 0.1399, G_loss : 4.8658\n",
      "Epoch [17/100], Step [1305/1582], D_loss : 0.0883, G_loss : 4.8624\n",
      "Epoch [17/100], Step [1405/1582], D_loss : 0.3334, G_loss : 4.4359\n",
      "Epoch [17/100], Step [1505/1582], D_loss : 0.0660, G_loss : 4.4579\n",
      "Epoch [18/100], Step [23/1582], D_loss : 0.4310, G_loss : 5.8243\n",
      "Epoch [18/100], Step [123/1582], D_loss : 0.2047, G_loss : 5.5551\n",
      "Epoch [18/100], Step [223/1582], D_loss : 0.0777, G_loss : 5.3361\n",
      "Epoch [18/100], Step [323/1582], D_loss : 0.0391, G_loss : 5.0626\n",
      "Epoch [18/100], Step [423/1582], D_loss : 0.0882, G_loss : 5.8362\n",
      "Epoch [18/100], Step [523/1582], D_loss : 0.0430, G_loss : 5.3427\n",
      "Epoch [18/100], Step [623/1582], D_loss : 0.0522, G_loss : 5.0204\n",
      "Epoch [18/100], Step [723/1582], D_loss : 1.4988, G_loss : 0.6076\n",
      "Epoch [18/100], Step [823/1582], D_loss : 0.3997, G_loss : 3.1146\n"
     ]
    }
   ],
   "source": [
    "dcgan.train(trainloader,epoch=50,check_step=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 애니메이션 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting a directory\n",
    "result_dir = './dcgan_result'\n",
    "\n",
    "# data load\n",
    "result_load = io.ImageCollection(result_dir + '/*.png')\n",
    "\n",
    "images=[]\n",
    "# image들을 gif파일로 저장\n",
    "for i, result in enumerate(result_load):\n",
    "        if i%5 ==0:\n",
    "            images.append(result)\n",
    "\n",
    "imageio.mimsave('generation_animation.gif',images,fps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
