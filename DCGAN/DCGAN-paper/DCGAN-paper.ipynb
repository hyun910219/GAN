{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "#### UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS\n",
    "\n",
    "### ABSTRACT\n",
    "최근에, CNN을 사용한 supervised learning(이하 SL)은 computer vision에서 좋은 성능을 보이고 있다. 상대적으로, CNN을 사용한 unsupervised learning(이하 UL)에서는 큰 관심을 받고 있지 않다. 우리는 이번에 SL과 UL간의 차이를 줄이려고 했다. 우리는 CNN 중 하나인 DCGAN을 소개할 것이다. DCGANS는 특정 구조적인 제약을 가졌고 강력한 UL의 모델이다. 다양한 데이터를 훈련하면서 우리는 Generator와 Discriminator에서 Deep convolutional adversarial pair은 물체의 계층적인 특징을 배운다는 것을 확인할 수 있었다. 추가적으로, 학습된 feature를 새로운 작업을 통해서 일반화된 image representation들에 적용할 수 있음을 증명했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INTRODUCTION\n",
    "여기 paper에서는 좋은 image representation을 만들기 위한 방법으로 GAN을 학습한 것을 제시한다. 그리고 후에 Generator와 Discriminator의 feature extractor를 활용해 supervised task를 시도한다. GAN은 maximum likelihood technique(ex. MCMC)에 대한 대안을 제안한다. 이것은 learning process와 heuristic cost function의 부족하다고 주장될 수 있다.  GAN은 train에서 불안정해서 종종 generator가  무의미한 결과를 생성해낸다. 최근 연구들에는 multi-layer GAN의 intermediate representation와 GAN의 이해화 시각화에 관해선 제한적이었다.\n",
    "\n",
    "이 paper에서는 다음과 같은 결과를 얻을 수 있었다.\n",
    "* Most settings에도 안정화된 학습을 할 수 있는 DCGAN을 만들었다.\n",
    "* 학습된 discriminator를 image classification task에 적용한 결과, UL과 비슷한 수준의 성능을 보였다.\n",
    "* 학습된 GAN의 filter를 시각화하고 특정 filter가 특정 object를 배운 것을 볼 수 있다.\n",
    "* Generated sample의 sematic quality를 쉽게 조작할 수 있는 산술적인 벡터 특징을 generator가 가진다.(Word2Vec과 비슷한 성질)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. RELATED WORK\n",
    "#### 2.1  REPRESENTATION LEARNING FROM UNLABELED DATA\n",
    "\n",
    " Unsupervised representation learning은 Computer vision에서 image의 context만큼 활발히 연구되고 있는 문제이다. Unsupervised representation learning에 대한 고전적인 방법은 data를 clustering(ex. K-means clustering)을 하고, 이 cluster를 classification score를 향상시키기 위해 활용되었다. image의 context에서 image의 patch들의 계층적인 cluster는 image representation을 잘 학습하게 할 수 있다. 또 다른 방법은 auto-encoer, ladder structure등을 학습하는게 있다. 이런 방법들은 image pixel들로 부터 좋은 feature representation을 배운 것을 알 수 있다. Deep belief network는 또한 계층적인 representation 학습에서 잘 작동하는 것을 보여왔다.\n",
    " \n",
    "\n",
    "#### 2.2 GENERATING NATURAL IMAGES\n",
    " \n",
    " Generative image models는 parametirc과 non-parametric의 두 범주로 나뉜다.\n",
    " \n",
    " Non-parametric model은 기존 이미지의 데이터와 일치시키는게 종종 일어난다. 그리고 image patch를 matching, texture synthesis, super-resolution,inpainting에 사용되어 왔다.\n",
    " \n",
    " 이미지를 생성하기 위한 parametric model은 광범위하게 연구가 되어지고 있다. 하지만, 실제의 Natural한 image를 만드는 것은 최근까지도 큰 성공을 거두지 못하고 있다. Variational sampling approach는 어느정도의 성공을 거뒀지만, sample들이 흐릿하게되는 문제가 있다. GAN은 이해할 수 없거나 noisy한 문제를 겪고 있다. Laplacian pyramid extension은 높은 이미지 해상도를 보였지만 여러 모델을 변경할 때 발생하는 noise 때문에 물체가 흔들리는 문제가 발생한다. RNN을 사용한 접근과 deconvolution network approach는 또한 natural image를 생성하는데 어느정도 성공을 거뒀다. 하지만, generator를  supervised task에 활용해보지 않았다.\n",
    " \n",
    "#### 2.3 VISUALIZING THE INTERNALS OF CNNS\n",
    " \n",
    "Neural network의 단점으로는 black box model로 인간이 이해하기 어려운것이다. CNN의 context에서 Ziler는 deconvolution과 maximal activation를 filterling하는 것을 사용하여 각 convolurion filter의 목적을 근사하게 찾을 수 있었다. 비슷하게, input의 gradien descent를 사용하면 filter의 특정 부분을 활성화하는 특징을 가지는 이미지를 검사할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. APPROACH AND MODEL ARCHITECTURE\n",
    "\n",
    "과거에 CNN을 사용한 GAN들은 성공적이지 못했다. 이것은 LAPGAN의 저자들이 저해상도의 생성된 이미지가 반복적으로 고해상도가 될 수 있는 모델을 위한 대안을 개발하게 만들었다.우리 또한 supervised literature에서 주로 사용되는 CNN구조를 사용한 GAN에 적용하는 어려움을 겪었다.하지만, extensive model exploration 후에 우리는 다양한 data에서도 stable하게 training하고 고해상도와 deeper generative model을 가능하게 하는 architecutre를 찾아냈다.\n",
    "\n",
    "approach의 핵심은 최근에 CNN 구조에 변화에 적용된 세가지를 적용하고 수정한것이다.\n",
    "\n",
    "\n",
    " <img src=\"./dcganimage/dcgan2.png\">\n",
    " \n",
    " \n",
    " 첫번째는 모든 Convolutional net에서 사용되는 deterministic spatial pooling function(such as maxpooling)을 strided convolution으로 대체하는 것이다. deterministic spatial pooling function은 network가 자신의 spatial downsampling하게 한다.(maxpooling은 max값만 가져와서 다른 정보는 사라져 down sampling이라고 볼 수 있다.) 하지만 strided convolution을 사용하는 것은 generator와 discriminator가 spatial upsampling을 학습할 수 있게 한다.\n",
    " \n",
    " 두번째는 가장 상위의 convolutional features에 연결되는 Fully connected layer들을 제거하는 추세를 반영했다. 그 중 가장 대표적인 방법은 global average pooling이다. 이 global average pooling은 최근의 image classification model에 이용되는 방법이다. global average pooling은 모델의 안정성을 향상시켰지만 convergence되는 속도를 늦췄다. 각각의 generator와 discriminator 각각의 input과 output의 가장 상위의 convolutional features를 직접적으로 연결하는데 중간적인 역할을 잘해냈다.\n",
    " \n",
    " Generator의 첫번째 레이어는 uniform noise distribution의 Z를 input으로 받는다. 그리고 단순한 matrix multiplication을 하는데 Fully connected라고 할 수 있지만, 결과적으로 4차원 텐서로 재구성되어 convolution stack의 시작으로 사용된다.(global average pooling에 대해서 조금더 공부하고 설명을 쓰자.) \n",
    " \n",
    " Discriminator에서 마지막 convolution layer를 flatten하고 sigmoid를 사용하여 하나의 값을 반환하는 output layer에 연결한다.(out = sigmoid(flatten(last convolution layer)*w+b)라고 보면된다.)\n",
    " \n",
    " 세번째는 Batch Normalization이다. Batch Norm은 각 input을 zero mean과 unit variance를 가지는 unit으로 Normalization해 training을 안정화하는 것이다. 이것은 poor initialization의 문제를 해결하는데 도움이 된다. 그리고 deeper model에서 gradient flow를 돕는다. 이것은 generator가 training을 할 때, 이전의 GAN에서 문제가 된 generator가 모든 sample이 하나의 point로 가는 것을 막을 수 있다.(예를 들면 mnist 데이터에서 generator가 1만 생성해내는 현상) 모든 layer에 직접적으로 Batch Norm을 연결하는 것은 model의 instability와 oscillation을 초래한다. Batch Norm을 generator의 output layer와 discriminator의 input layer에 적용하지 않아 문제를 피했다.\n",
    "\n",
    " ReLU activation은 generator에서 output layer만 제외하고 사용됐다. output layer에서는 tanh function이 사용됐다. 이 bounded function(tanh)을 사용하는 것은 model을 빠르게 학습하고 training distribution의 color space를 복구한다.\n",
    " \n",
    " Discriminator에서는 leakyReLU activation이 잘 작동했는데 특히 고해상도 modeling에서 성능이 좋았다. 이런 부분은 maxout을 사용한 original GAN과는 대조되는 점이다.\n",
    " \n",
    "  <img src=\"./dcganimage/dcgan1.png\">\n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. DETAILS OF ADVERSARIAL TRAINING\n",
    "\n",
    "여기서는 DCGAN을 Large-scale Scene Understanding(LSUN), Imagenet-1k , Faces dataset인 3가지 종류의 data set을 학습했다.\n",
    "\n",
    " training image를 tanh activation의 범위인 [-1,1]로 스케일링 하는것 이외에는 사전처리를 진행하지 않았다. 모든 모델들은 minibatch 크기가 128인 minibatch SGD으로 학습했다. 모든 weight들은 평균이 0이고 편차가 0.02인 Normal distribution으로 부터 초기화했다. LeakyReLU에서는, 모든 모델에서 leak을 0.2로 설정했다.\n",
    " \n",
    " 이전 GAN에서는 momentum을 사용한 반면에, 여기에서는 Adam optimizer를 사용했다. 그리고 learning rate의 경우에는 0.001은 너무 크기 때문에 0.0002를 대신에 사용했다. 추가적으로, momentum의 $\\beta_1$이 0.9일 때는 training할 때 oscillation과 불안정해서 0.5로 바꿨다.\n",
    " \n",
    "#### 4.1 LSUN\n",
    "\n",
    " Generative image model로 부터의 sample의 질이 향상되어서 훈련 데이터에 대해 overfitting과 memorization에 대한 우려가 커졌다. 더 많은 데이터와 고화질 생성으로 모델이 어떻게 변하는지 보여주기 위해서, 300만을 조금 넘는 LSUN bedroos 훈련 예제로 부터 모델을 학습했다. 최근 분석에서는 빠른 모델 학습과 일반화된 성능간에 직접적인 관계가 있음을 보이고 있다. 여기에서는 수렴했을 때의 sample 뿐만아니라 , online learning을 모방하는 1 epoch의 sample을 보여준다. 이것은 모델이 단순히 training example을 overfitting하거나 기억해서 고해상의 샘플을 생성하는게 아니라는 것을 보여줄 수 있다. augmentation은 image에 적용하지 않았다.\n",
    " \n",
    " <img src=\"./dcganimage/dcgan3.png\">\n",
    "\n",
    "##### 4.1.1 DEDUPLICATION\n",
    " \n",
    " Input example을 generator가 기억할 가능성을 줄이기 위해서 간단한 image de-duplication process를 수행했다. 여기에서는 3072-128-3072 de-noising dropout regularized ReLU autoencoder를 training example에서 32*32 center crop으로 downsample한 imagee에 대해 적합했다. 결과적으로 code layer(128) 활성화는 효과적이 정보 보존 기술로 입증된 ReLU 활성화를 threshold를 통해서 이진화되고, semantic hashing에 편리한 형태를 제공하고 선형시간 중복제거를 허용한다. 해시 충돌에 대해서 실제 눈으로 조사하였는데 100건 중 1건만 잘못되어 높은 정밀도를 보였다. 추가적으로, 이 기술을 활용하여 약 275,000개에 가까운 복제품을 제거했다.\n",
    " \n",
    " 짧게 말하면 de-duplication이라는 기술로 autoencoder를 적합시키고 code layer의 값들을 해싱 방법을 사용해서 중복을 제거했다라고 볼 수 있다.\n",
    "\n",
    "#### 4.2 FACE\n",
    " 여기 저자들은 random web image에 사람이름을 쿼리로 사용해서 사람의 얼굴을 포함한 사진을 모았다. OpenCV face detector를 사용하여  충분히 고해상도의 35만 face box를 얻어 훈련에 사용했다.특별한 augmentation은 적용하지 않았다.\n",
    " \n",
    "#### 4.3 IMAGENET-1K\n",
    " 여기 저자들은 Imagenet-1k 데이터를 사용했다. center를 32*32로 crop하여 훈련했다. 특별한 augmentation은 적용하지 않았다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  EMPIRICAL VALIDATION OF DCGANs CAPABILITIES\n",
    "\n",
    "#### 5.1 CLASSIFYING CIFAR-10 USING GANS AS A FEATURE EXTRACTOR\n",
    "\n",
    " UL 알고리즘의 성능을 평가하는 방법 중 하나는 supervised dataset의 feature extractor에 적용해 보는것이다. 추출된 feature들의 top을 linear model에 적합시켜 성능을 평가한다.\n",
    " \n",
    " CIFAR-10 데이터셋에서 K-means를 feature learning algorithm으로 사용해 잘 조정된 sigle layer feature extraction pipline에서서 강한 baseline을 입증됐습니다. 매우 많은 양의 feature maps(4800)을 사용했을 때 80.6%의 accruacy의 성능을 보였다. base algorithm의 UL multi-layered extension은 82%의 accuracy를 보였다. Supervised task에 DCGAN이 학습한 representation들의 성능을 평가하기 위해서 Imagenet-1k를 학습했다. 그리고 discriminator에서 4*4spatial grid를 만들기 위해서 모든 layer에 maxpooling을 한 convolutional feature를 사용했다. 이러한 feature들은 flatten과 concatenated가 되어 28672차원을 가지는 vector가 된다. 그리고 이것들의 top을 regularized linear L2-SVM classifier를 훈련한다. 그 결과, 82.8% accuracy를 보였고 이 성능은 K-means를 기반으로 하는 방법들 보다 뛰어나다.\n",
    " \n",
    "특히, discriminator는 k-means에 기반한 방법에 비교하여 feature map(512 in the highest layer)이 작다.그러나 결과적으로 4*4 spatial location의 많은 layer 때문에 더 큰 전체 feature vector size를 가진다. DCGAN의 성능은 CNN들 보다 나은 성능을 보이지는 못하지만, 일반적인 CNN과 다르게 unsupervised의 방식으로 훈련을 하여 특별하다. 그리고 discriminator의 representation을 finetunning하여 더 향상 시킬 수 있지만, 여기에서는 실시하지 않았다. 추가적으로, 여기에서 학습된 DCGAN은 CIFAR-10을 학습을 할 때 사용하지 않았는데 이것은 어떤 도메인에서도 적용할 수 있음을 나타낸다.\n",
    "\n",
    "<img src=\"./dcganimage/dcgan4.png\">\n",
    "\n",
    "#### 5.2 CLASSIFYING SVHN DIGITS USING GANs AS A FEATURE EXTRACTOR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. INVESTIGATING AND VISUALIZING THE INTERNALS OF THE NETWORKS\n",
    "\n",
    " 저자들은 다양한 방법으로 학습된 generator와 discriminator를 조사했다. 학습 데이터에서 어떤 nearest neighbor search를 하지 않았다. pixel 또는 featur space의 Nearest neighbor는 작은 이미지 변환에 대해서 좋은 방법이 아니다. 저자들은 좋지않은 metric인 log-likelihood또한 사용하지 않았다.\n",
    " \n",
    " 대신 다음을 통해서 잘 학습된것을 보여준다.\n",
    " \n",
    "#### 6.1 WALKING IN THE LATENT SPACE\n",
    " 가장 첫 실험은 latent space에 대해서 이해하는 것이 었습니다. latent space를 다양하게 지나가면서 memorization(sharp transition)이 발생하지 않음을 알 수 있다. 단지  model이 train data를 memorization을 했다면 sharp transition이 발생할 것입니다. 하지만 지나갈 때 생성된 이미지에서 semantic change가 발생한다면, model이 memorization을 하지 않고 중요한 representation을 학습했다고 판단할 수 있다.\n",
    " \n",
    " <img src=\"./dcganimage/dcgan5.png\">\n",
    "\n",
    "위의 그림은 latent space를 조금씩 바꿔가며 관찰한 결과이다. sharp transition이 발생하지 않고 서서히 변화하는 것을 알 수 있다.\n",
    "\n",
    "#### 6.2 VISUALIZING THE DISCRIMINATOR FEATURES\n",
    " SL에서의 CNN은 많은 image 데이터에서 feature를 아주 잘 학습을 하는 것은 증명됐다(Zeliler & Fergus,2014). 추가적으로 object detect 또한 학습이 된다. 여기에서는 unsupervised DCGAN도 많은 image 데이터에서 학습이 된다면 계층적인 feature를 배운것을 알 수 있다. (Springenberg et al.,2014)에서 제안된 guided backpropagation을 사용하여 discriminator에서 습된 feature가 bedroos, beds, window의 부분에 대해서 활성화 되는것을 볼 수 있다.\n",
    " \n",
    "  <img src=\"./dcganimage/dcgan6.png\">\n",
    "  \n",
    " 위 그림을 보면 layer에서 사용된 필터가 물체들의 특징을 잘 표현하고 있음을 알 수 있다.\n",
    " \n",
    "#### 6.3 MANIPULATING THE GENERATOR REPRESENTATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
