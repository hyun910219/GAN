{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. importt libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_set = dset.MNIST('./data',train=True,transform=transforms.ToTensor(),\n",
    "                     target_transform=None,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468\n"
     ]
    }
   ],
   "source": [
    "data = torch.utils.data.DataLoader(data_set, batch_size=128,shuffle=True,\n",
    "                                          num_workers=0,drop_last=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "# [z+onehot vector] -> [28*28*1]\n",
    "# z is vector of 100 rnadom numbers(0~1)\n",
    "# one-hot vector size is 10\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self,dim_z=100,dim_c = 10,input_size=28,out_dim=16,colch=1,leaky=0.2):\n",
    "        super(G,self).__init__()\n",
    "        \n",
    "        self.dim_z = dim_z\n",
    "        self.dim_c = dim_c\n",
    "        self.input_size = input_size\n",
    "        self.out_dim = out_dim\n",
    "        self.colch = colch\n",
    "        self.leaky = leaky\n",
    "        \n",
    "        \n",
    "        self.fc = deconv(dim_c+dim_z,out_dim*16,int(input_size/4),1,0,bn=False) # [batch,110] -> [batch,256,7,7]\n",
    "        self.layer1 = nn.Sequential(OrderedDict([ \n",
    "            ('conv1',nn.ConvTranspose2d(out_dim*16,out_dim*8,3,2,1)),  # [batch,256,7,7] -> [batch,128,14,14]\n",
    "            ('relu1',nn.LeakyReLU(negative_slope=0.2)),\n",
    "            ('bn1',nn.BatchNorm2d(128)),\n",
    "            ('conv2',nn.ConvTranspose2d(out_dim*8,out_dim*4,3,1,1)) , # [batch,128,14,14] -> [batch,64,14,14]\n",
    "            ('relu2',nn.LeakyReLU(negative_slope=0.2)),\n",
    "            ('bn2',nn.BatchNorm2d(64))         \n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([ \n",
    "            ('conv3',nn.ConvTranspose2d(out_dim*4,out_dim*1,3,2,1)),  # [batch,64,14,14] -> [batch,16,28,28]\n",
    "            ('relu3',nn.LeakyReLU(negative_slope=0.2)),\n",
    "            ('bn3',nn.BatchNorm2d(16)),\n",
    "            ('conv4',nn.ConvTranspose2d(out_dim*1,1,3,1,1)) , # [batch,16,28,28] -> [batch,1,28,28]\n",
    "            ('relu4',nn.LeakyReLU(negative_slope=0.2))   \n",
    "        ]))\n",
    "\n",
    "        \n",
    "    def forward(self,z,c):\n",
    "        z_c = torch.cat([z,c],1) \n",
    "        z_c = z_c.view(z_c.size(0),z_c.size(1),1,1)\n",
    "        out = self.fc(z_c)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = F.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "# [28*28*1] -> output variable (real data or not), one-hot vector(label)\n",
    "# output variable is one number(0~1)\n",
    "# one-hot vector size is 10\n",
    "\n",
    "class D(nn.Module):\n",
    "    def __init__(self,input_size=28,colch=1,conv_dim=16):\n",
    "        super(D,self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.colch = colch\n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "            ('conv1',nn.Conv2d(1,16,3,padding=1)), # [batch,1,28,28] -> [batch,16,28,28]\n",
    "            ('relu1',nn.ReLU()),\n",
    "            ('bn1',nn.BatchNorm2d(16)),\n",
    "            ('conv2',nn.Conv2d(16,32,4,2,padding=0)), # [batch,16,28,28] -> [batch,32,14,14]\n",
    "            ('relu2',nn.ReLU()),\n",
    "            ('bn2',nn.BatchNorm2d(32))\n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "            ('conv3',nn.Conv2d(32,64,3,padding=1)), # [batch,32,14,14] -> [batch,64,14,14]\n",
    "            ('relu3',nn.ReLU()),\n",
    "            ('bn3',nn.BatchNorm2d(64)),\n",
    "            ('conv4',nn.Conv2d(64,128,4,2,padding=0)), # [batch,64,14,14] -> [batch,128,7,7]\n",
    "            ('relu4',nn.ReLU()),\n",
    "            ('bn4',nn.BatchNorm2d(128)),\n",
    "        ]))\n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.ConvTranspose2d(128,1,7,1,0),\n",
    "                                nn.Sigmoid())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(128*7*7,10),\n",
    "                                nn.ReLU())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        print(out.size())\n",
    "        out = self.layer2(out)\n",
    "        output = self.fc1(out).squeeze()\n",
    "        print(out.size())\n",
    "        flat = out.view(out.size(0),-1)\n",
    "        print(flat)\n",
    "        label = self.fc2(flat)\n",
    "        return output, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put class instance on multi gpu\n",
    "\n",
    "generator = G().cuda()\n",
    "discriminator = D().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put labels on multi gpu\n",
    "\n",
    "ones_label = Variable(torch.ones(128,1)).cuda()\n",
    "zeros_label = Variable(torch.zeros(128,1)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function and optimizer \n",
    "# this time, use LSGAN loss(https://arxiv.org/abs/1611.04076v2)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "gen_optim = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
    "dis_optim = torch.optim.Adam(discriminator.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define integer to onehot vector function\n",
    "# 3 -> [0 0 0 1 0 0 0 0 0 0]\n",
    "\n",
    "def int_to_onehot(z_label):\n",
    "    one_hot_array = np.zeros(shape=[len(z_label), discrete_latent_size])\n",
    "    one_hot_array[np.arange(len(z_label)), z_label] = 1\n",
    "    return one_hot_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "\n",
    "epoch = 1000\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_gpus = 1\n",
    "discrete_latent_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32, 11, 11])\n",
      "torch.Size([128, 128, 4, 4])\n",
      "Variable containing:\n",
      " 7.5565e-02  1.2153e-02  7.1812e-02  ...  -2.1617e-01  2.0515e-01 -2.1617e-01\n",
      "-8.8837e-02 -8.8837e-02  1.4595e-01  ...  -1.1095e-01  7.2284e-02 -2.1617e-01\n",
      "-8.8837e-02 -8.8837e-02 -8.8837e-02  ...   9.6627e-03 -2.1617e-01 -2.1617e-01\n",
      "                ...                   â‹±                   ...                \n",
      "-3.3884e-02 -5.4901e-02 -4.7375e-02  ...   9.1787e-01  1.0589e+00 -2.1617e-01\n",
      "-8.8837e-02 -8.8837e-02 -1.4540e-02  ...  -2.1617e-01 -2.1617e-01 -2.1617e-01\n",
      "-6.7852e-02 -8.8837e-02 -8.8837e-02  ...   5.1000e-02  2.8283e-01 -2.1617e-01\n",
      "[torch.cuda.FloatTensor of size 128x2048 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch at d:\\projects\\pytorch\\torch\\lib\\thc\\generic/THCTensorMathBlas.cu:243",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-320e0fbf94e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mgen_optim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mgen_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_random\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz_label_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mdis_fake\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mgen_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdis_fake\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mones_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m+\u001b[0m \u001b[0mdiscrete_latent_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_fake\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz_label_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mgen_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-169-a9edcd448dce>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mflat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36maddmm\u001b[1;34m(cls, *args)\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36m_blas\u001b[1;34m(cls, args, inplace)\u001b[0m\n\u001b[0;32m    918\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\_functions\\blas.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, add_matrix, matrix1, matrix2, alpha, beta, inplace)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         return torch.addmm(alpha, add_matrix, beta,\n\u001b[1;32m---> 26\u001b[1;33m                            matrix1, matrix2, out=output)\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch at d:\\projects\\pytorch\\torch\\lib\\thc\\generic/THCTensorMathBlas.cu:243"
     ]
    }
   ],
   "source": [
    "# train \n",
    "\n",
    "for i in range(100):\n",
    "    for j,(image,label) in enumerate(data):\n",
    "        \n",
    "        # put image & label on gpu\n",
    "        \n",
    "        image = Variable(image).cuda()\n",
    "        label = torch.from_numpy(int_to_onehot(label.numpy()))\n",
    "        label = Variable(label.type_as(torch.FloatTensor())).cuda()\n",
    "    \n",
    "        # generator \n",
    "        \n",
    "        for k in range(2):\n",
    "            z_random = np.random.rand(batch_size,100)\n",
    "            z_label = np.random.randint(0, 10, size=batch_size)\n",
    "            z_label_onehot = int_to_onehot(z_label)\n",
    "            \n",
    "            # change first 10 labels from random to 0~9          \n",
    "            for l in range(10):\n",
    "                z_label[l]=l\n",
    "            \n",
    "            # preprocess z\n",
    "            z_random = torch.from_numpy(z_random).type_as(torch.FloatTensor())\n",
    "            z_random = Variable(z_random).cuda()\n",
    "            z_label_onehot = torch.from_numpy(z_label_onehot).type_as(torch.FloatTensor())\n",
    "            z_label_onehot = Variable(z_label_onehot).cuda()\n",
    "\n",
    "            # calculate loss and apply gradients\n",
    "            # gen_loss = gan loss(fake) + categorical loss\n",
    "            gen_optim.zero_grad()\n",
    "            gen_fake = generator.forward(z_random,z_label_onehot)\n",
    "            dis_fake,label_fake = discriminator.forward(gen_fake)\n",
    "            gen_loss = torch.sum(loss_func(dis_fake,ones_label)) \\\n",
    "                      + discrete_latent_size * torch.sum(loss_func(label_fake,z_label_onehot))\n",
    "            gen_loss.backward(retain_variables=True)\n",
    "            gen_optim.step()\n",
    "\n",
    "        # discriminator\n",
    "        # dis_loss = gan_loss(fake & real) + categorical loss\n",
    "        dis_optim.zero_grad()\n",
    "        dis_real, label_real = discriminator.forward(image)\n",
    "        dis_loss = torch.sum(loss_func(dis_fake,zeros_label)) \\\n",
    "                  + torch.sum(loss_func(dis_real,ones_label)) \\\n",
    "                  + discrete_latent_size * torch.sum(loss_func(label_real,label))\n",
    "        dis_loss.backward()\n",
    "        dis_optim.step()\n",
    "    \n",
    "    # model save\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_label_onehot.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6272"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
