{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고  :https://github.com/znxlwm/tensorflow-MNIST-GAN-DCGAN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, imageio, itertools, pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "data shape :  (55000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "train_set = mnist.train.images\n",
    "mean_train = np.mean(train_set)\n",
    "std_train =np.std(train_set)\n",
    "train_set = (train_set - 0.5)/0.5\n",
    "\n",
    "print(\"data shape : \", train_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu 함수\n",
    "def relu_layer(x,node,std,name=\"relu_layer\",drop_out=0.0):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(\"w\",[x.get_shape()[1],node],initializer=tf.truncated_normal_initializer(mean=0.,stddev=std))\n",
    "        b = tf.get_variable(\"b\",[node],initializer=tf.constant_initializer(0.))\n",
    "        if drop_out!=0:\n",
    "            return tf.nn.dropout(tf.nn.relu(tf.matmul(x,w)+b),drop_out)\n",
    "        else:\n",
    "            return tf.nn.relu(tf.matmul(x,w)+b)\n",
    "        \n",
    "\n",
    "        \n",
    "# 훈련된 과정을 볼 수 있는 함수.        \n",
    "def show_train_hist(hist,show=False,save=False,path='Train_hist.png'):\n",
    "    x = range(len(hist['D_losses']))\n",
    "    \n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "    plt.subplots(1,1,figsize=(5,5))\n",
    "    plt.plot(x,y1,label='D_loss')\n",
    "    plt.plot(x,y2,label='G_loss')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self,sess,input_size=784,output_size=784,\n",
    "                 batch_size = 128,z_dim = 100,\n",
    "                 gf_dim = 254,df_dim=1024):\n",
    "        '''\n",
    "        Args:\n",
    "            sess : Tensorflow session\n",
    "            batch_size : The size of batch\n",
    "            z_dim : Dimension of \n",
    "            gf_dim : Dimension of generator filters in first genrator layers\n",
    "            df_dim : Dimension of discriminator filters in first discriminator layers\n",
    "            \n",
    "        '''\n",
    "        self.sess = sess\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = 128\n",
    "        self.z_dim = z_dim\n",
    "        self.gf_dim = gf_dim\n",
    "        self.df_dim = df_dim\n",
    "        self.build_model()\n",
    "\n",
    "    # model을 생성한다.\n",
    "    def build_model(self):\n",
    "\n",
    "        self.x = tf.placeholder('float32',[self.batch_size,self.input_size],'real_x')\n",
    "        self.z = tf.placeholder('float32',[None,self.z_dim],'z')\n",
    "        self.drop_out = tf.placeholder('float32',name='drop_out')\n",
    "        \n",
    "        self.G = self.generator(self.z)\n",
    "        self.D_real = self.discriminator(self.x,self.drop_out,reuse=False)\n",
    "        self.D_fake = self.discriminator(self.G,self.drop_out,reuse=True)\n",
    "        \n",
    "\n",
    "        eps = 1e-2\n",
    "        self.d_loss = tf.reduce_mean(-tf.log(self.D_real+eps)-tf.log(1-self.D_fake+eps))\n",
    "        self.g_loss = tf.reduce_mean(-tf.log(self.D_fake+eps))\n",
    "        \n",
    "        t_vars = tf.trainable_variables()\n",
    "        \n",
    "        self.d_vars = [var for var in t_vars if 'discriminator' in var.name]\n",
    "        self.g_vars = [var for var in t_vars if 'generator' in var.name]\n",
    "        \n",
    "    # training\n",
    "    def train(self,data,epoch=50,learning_rate=0.0002,beta1=0.9,drop_out=0.5):\n",
    "        \n",
    "        self.data = data\n",
    "        self.learning_rate = learning_rate\n",
    "        self.bata1 = beta1\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        self.d_optim = tf.train.AdamOptimizer(learning_rate,beta1=beta1).minimize(self.d_loss,var_list = self.d_vars)\n",
    "        self.g_optim = tf.train.AdamOptimizer(learning_rate,beta1=beta1).minimize(self.g_loss,var_list = self.g_vars)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        sample_z = np.random.normal(0,1,(25,100))\n",
    "        iter_per_epoch = len(self.data) // self.batch_size\n",
    "        counter =1 \n",
    "        self.epoch = epoch\n",
    "        self.train_hist={}\n",
    "        self.train_hist['D_losses']=[]\n",
    "        self.train_hist['G_losses']=[]\n",
    "        self.train_hist['per_epoch_ptimes'] = []\n",
    "        self.train_hist['total_ptime']=[]\n",
    "        self.train_hist['epoch'] = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # results save folder\n",
    "        if not os.path.isdir(\"GAN_results\"):\n",
    "            os.mkdir('GAN_results')\n",
    "        if not os.path.isdir(\"GAN_results/images\"):\n",
    "            os.mkdir(\"GAN_results/images\")\n",
    "\n",
    "        for e in range(self.epoch):\n",
    "            epoch_start_time =time.time()\n",
    "            for idx in range(iter_per_epoch):\n",
    "                start_ptime = time.time()\n",
    "                batch_images = self.data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_z = np.random.normal(0,1,(self.batch_size,100))\n",
    "                \n",
    "                # update D network\n",
    "                _, d_loss = self.sess.run([self.d_optim,self.d_loss],feed_dict={self.x:batch_images,self.z:batch_z,self.drop_out:drop_out})\n",
    "                \n",
    "                # update G netework\n",
    "                _, g_loss = self.sess.run([self.g_optim,self.g_loss],feed_dict={self.z:batch_z,self.drop_out:drop_out})\n",
    "                counter+=1\n",
    "                \n",
    "\n",
    "                        \n",
    "            print(\"Epoch:[%2d] time: %4.4f, d_loss : %.8f, g_loss: %.8f\"%\n",
    "                    (e,time.time()-epoch_start_time,d_loss,g_loss))\n",
    "            \n",
    "            self.train_hist['D_losses'].append(d_loss)\n",
    "            self.train_hist['G_losses'].append(g_loss)\n",
    "            self.train_hist['per_epoch_ptimes'].append(time.time()-epoch_start_time)\n",
    "            self.train_hist['total_ptime'].append(time.time()-start_time)\n",
    "            self.train_hist['epoch']\n",
    "            \n",
    "            save_image = self.sess.run(self.G,feed_dict={self.z:sample_z})\n",
    "            \n",
    "            save_image = np.reshape(save_image,[25,int(np.sqrt(self.input_size)),int(np.sqrt(self.input_size))])\n",
    "            \n",
    "            # 표시할 그림 수 \n",
    "            size_figure_grid=5\n",
    "            # fig는 나타내는 큰 틀\n",
    "            # ax는 fig에 포함된 그림\n",
    "            fig,ax = plt.subplots(size_figure_grid,size_figure_grid,figsize=(5,5))\n",
    "            # ax에서 눈금을 제거\n",
    "            for i,j in itertools.product(range(size_figure_grid),range(size_figure_grid)):\n",
    "                ax[i,j].get_yaxis().set_visible(False)\n",
    "                ax[i,j].get_yaxis().set_visible(False)\n",
    "            \n",
    "            # 그림을 나타내는 순서\n",
    "            for k in range(5*5):\n",
    "                i = k // 5\n",
    "                j = k % 5\n",
    "                ax[i,j].cla()\n",
    "                ax[i,j].imshow(np.reshape(save_image[k],(28,28)),cmap='gray')\n",
    "\n",
    "            path = './GAN_results/images/'\n",
    "            label = 'Epoch {0}'.format(e)\n",
    "            fig.text(0.5,0.04,label,ha='center')\n",
    "            plt.savefig(path+str(e)+'.jpg')\n",
    "            plt.close()\n",
    "            \n",
    "    def result(self):\n",
    "        # loss function의 변화를 graph로 저장\n",
    "        with open(\"GAN_results/train_hist.pkl\",'wb') as f:\n",
    "            pickle.dump(self.train_hist,f)\n",
    "        show_train_hist(self.train_hist, save=True, path='GAN_results/GAN_train_hist.png')\n",
    "\n",
    "        self.images=[]\n",
    "        # image들을 gif파일로 저장\n",
    "        for e in range(self.epoch):\n",
    "            image_name = './GAN_results/images/'+str(e)+'.jpg'\n",
    "            self.images.append(imageio.imread(image_name))\n",
    "\n",
    "        imageio.mimsave('GAN_results/generation_animation.gif',self.images,fps=10)\n",
    "\n",
    "        \n",
    "    def generator(self,z):\n",
    "        with tf.variable_scope('generator'):\n",
    "            # hidden 1\n",
    "            hidden1 = relu_layer(x = z,node = 254, std = 0.02,name=\"G_h1\")\n",
    "\n",
    "            # hidden 2\n",
    "            hidden2 = relu_layer(x = hidden1,node = 512,std = 0.02,name=\"G_h2\")\n",
    "\n",
    "            # hidden 3\n",
    "            hidden3 = relu_layer(x = hidden2,node = 1024,std = 0.02,name=\"G_h3\")\n",
    "\n",
    "            # hidden 4\n",
    "            with tf.variable_scope(\"G_out\"):\n",
    "                w = tf.get_variable('w',[hidden3.get_shape()[1],784],initializer=tf.truncated_normal_initializer(mean=0,stddev=0.02))\n",
    "                b = tf.get_variable(\"b\",[1],initializer = tf.constant_initializer(0.))\n",
    "                out = tf.nn.tanh(tf.matmul(hidden3,w)+b)\n",
    "            return out        \n",
    "    \n",
    "    def discriminator(self,x,drop_out,reuse=False):\n",
    "        with tf.variable_scope('discriminator') as scope:\n",
    "            if reuse:\n",
    "                scope.reuse_variables()\n",
    "                \n",
    "            # hidden 1\n",
    "            hidden1 = relu_layer(x = x,node = self.df_dim,std = 0.02,drop_out=drop_out,name='D_h1')\n",
    "\n",
    "            # hidden 2\n",
    "            hidden2 = relu_layer(x = hidden1,node = int(self.df_dim/2),std = 0.02,drop_out=drop_out,name=\"D_h2\")\n",
    "\n",
    "            # hidden 3\n",
    "            hidden3 = relu_layer(x = hidden2,node = int(self.df_dim/4),std = 0.02,drop_out = drop_out,name='D_h3')\n",
    "\n",
    "            # hdden 4\n",
    "            with tf.variable_scope(\"D_out\"):\n",
    "                w = tf.get_variable(\"w\",[hidden3.get_shape()[1],1],initializer=tf.truncated_normal_initializer(mean=0,stddev=0.02))\n",
    "                b = tf.get_variable(\"b\",[1],initializer = tf.constant_initializer(0.))\n",
    "                out = tf.sigmoid(tf.matmul(hidden3,w)+b)\n",
    "\n",
    "            return out\n",
    "                             \n",
    "                        \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 256\n",
    "learning_rate = 0.0002\n",
    "train_epoch = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[ 0] time: 4.0415, d_loss : 0.25609130, g_loss: 3.18347621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py:524: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[ 1] time: 3.7699, d_loss : 0.05877637, g_loss: 3.78600621\n",
      "Epoch:[ 2] time: 3.9666, d_loss : -0.01364723, g_loss: 4.52642441\n",
      "Epoch:[ 3] time: 3.8861, d_loss : 0.10865010, g_loss: 3.32130837\n",
      "Epoch:[ 4] time: 3.8401, d_loss : 0.25496328, g_loss: 3.05040359\n",
      "Epoch:[ 5] time: 3.8045, d_loss : 0.11699235, g_loss: 2.98142338\n",
      "Epoch:[ 6] time: 3.9359, d_loss : 0.38123462, g_loss: 2.56508470\n",
      "Epoch:[ 7] time: 3.8210, d_loss : 0.46453840, g_loss: 2.43334985\n",
      "Epoch:[ 8] time: 3.8285, d_loss : 0.49145687, g_loss: 2.99119020\n",
      "Epoch:[ 9] time: 3.9799, d_loss : 0.39742225, g_loss: 2.33363533\n",
      "Epoch:[10] time: 3.9547, d_loss : 0.51886845, g_loss: 2.33598852\n",
      "Epoch:[11] time: 3.8762, d_loss : 0.26577196, g_loss: 2.65595555\n",
      "Epoch:[12] time: 3.8673, d_loss : 0.40984327, g_loss: 2.69819736\n",
      "Epoch:[13] time: 3.9042, d_loss : 0.51845384, g_loss: 2.13338947\n",
      "Epoch:[14] time: 3.7864, d_loss : 0.47256237, g_loss: 2.45844460\n",
      "Epoch:[15] time: 3.8526, d_loss : 0.55647147, g_loss: 2.15216899\n",
      "Epoch:[16] time: 3.9184, d_loss : 0.47526243, g_loss: 2.54062700\n",
      "Epoch:[17] time: 4.0015, d_loss : 0.70722783, g_loss: 1.67075062\n",
      "Epoch:[18] time: 4.0076, d_loss : 0.63277400, g_loss: 1.68084168\n",
      "Epoch:[19] time: 3.8278, d_loss : 0.95126641, g_loss: 1.62859750\n",
      "Epoch:[20] time: 3.8779, d_loss : 0.97403193, g_loss: 1.65265357\n",
      "Epoch:[21] time: 3.8548, d_loss : 1.02071035, g_loss: 1.30278397\n",
      "Epoch:[22] time: 3.9048, d_loss : 0.92879736, g_loss: 1.24814367\n",
      "Epoch:[23] time: 3.9405, d_loss : 0.96458548, g_loss: 1.07765007\n",
      "Epoch:[24] time: 3.9366, d_loss : 0.91980004, g_loss: 1.54339421\n",
      "Epoch:[25] time: 3.8786, d_loss : 1.01298237, g_loss: 1.28076220\n",
      "Epoch:[26] time: 3.9735, d_loss : 1.00628102, g_loss: 0.88503218\n",
      "Epoch:[27] time: 3.8358, d_loss : 1.02704000, g_loss: 1.04801822\n",
      "Epoch:[28] time: 3.8217, d_loss : 1.07382238, g_loss: 1.15766895\n",
      "Epoch:[29] time: 4.0122, d_loss : 0.99913216, g_loss: 1.07072198\n",
      "Epoch:[30] time: 3.9755, d_loss : 1.02978659, g_loss: 0.92978632\n",
      "Epoch:[31] time: 3.9858, d_loss : 1.04314780, g_loss: 1.07187665\n",
      "Epoch:[32] time: 3.8005, d_loss : 1.10611737, g_loss: 0.89453280\n",
      "Epoch:[33] time: 3.9089, d_loss : 1.12729526, g_loss: 0.88237607\n",
      "Epoch:[34] time: 3.7832, d_loss : 1.08516443, g_loss: 0.98064518\n",
      "Epoch:[35] time: 3.8384, d_loss : 1.16398263, g_loss: 1.00778329\n",
      "Epoch:[36] time: 3.8189, d_loss : 1.16639388, g_loss: 0.98851871\n",
      "Epoch:[37] time: 3.8919, d_loss : 1.12937772, g_loss: 0.91109329\n",
      "Epoch:[38] time: 3.8388, d_loss : 1.13158154, g_loss: 1.13941526\n",
      "Epoch:[39] time: 3.9614, d_loss : 1.25028992, g_loss: 0.74268210\n",
      "Epoch:[40] time: 3.8279, d_loss : 1.13888943, g_loss: 1.02409327\n",
      "Epoch:[41] time: 3.7796, d_loss : 1.28113818, g_loss: 0.99007177\n",
      "Epoch:[42] time: 3.9117, d_loss : 1.33114529, g_loss: 0.80717975\n",
      "Epoch:[43] time: 3.8321, d_loss : 1.23514938, g_loss: 0.88474911\n",
      "Epoch:[44] time: 3.8073, d_loss : 1.23269784, g_loss: 0.84900331\n",
      "Epoch:[45] time: 3.8260, d_loss : 1.21251082, g_loss: 0.93846065\n",
      "Epoch:[46] time: 3.8904, d_loss : 1.13801742, g_loss: 0.88490623\n",
      "Epoch:[47] time: 3.8429, d_loss : 1.24784660, g_loss: 0.84094149\n",
      "Epoch:[48] time: 4.0275, d_loss : 1.27147651, g_loss: 0.84160048\n",
      "Epoch:[49] time: 3.8197, d_loss : 1.18676281, g_loss: 0.71958798\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "gan = GAN(sess=tf.Session())\n",
    "\n",
    "gan.train(train_set)\n",
    "\n",
    "gan.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
